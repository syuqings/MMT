transformer: encoder.embed.embed.weight, shape=torch.Size([7229, 512]), num:3701248
transformer: encoder.pe.pe, shape=torch.Size([1, 200, 512]), num:102400
transformer: encoder.vis_embed.weight, shape=torch.Size([512, 2048]), num:1048576
transformer: encoder.vis_embed.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.txt_encoder.norm_1.alpha, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.txt_encoder.norm_1.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.txt_encoder.norm_2.alpha, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.txt_encoder.norm_2.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.txt_encoder.attn.q_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.0.txt_encoder.attn.q_linear.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.txt_encoder.attn.v_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.0.txt_encoder.attn.v_linear.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.txt_encoder.attn.k_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.0.txt_encoder.attn.k_linear.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.txt_encoder.attn.out.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.0.txt_encoder.attn.out.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.txt_encoder.ff.linear_1.weight, shape=torch.Size([2048, 512]), num:1048576
transformer: encoder.layers.0.txt_encoder.ff.linear_1.bias, shape=torch.Size([2048]), num:2048
transformer: encoder.layers.0.txt_encoder.ff.linear_2.weight, shape=torch.Size([512, 2048]), num:1048576
transformer: encoder.layers.0.txt_encoder.ff.linear_2.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.vis_encoder.norm_1.alpha, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.vis_encoder.norm_1.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.vis_encoder.norm_2.alpha, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.vis_encoder.norm_2.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.vis_encoder.attn.mean_rho, shape=torch.Size([8, 1]), num:8
transformer: encoder.layers.0.vis_encoder.attn.mean_theta, shape=torch.Size([8, 1]), num:8
transformer: encoder.layers.0.vis_encoder.attn.precision_rho, shape=torch.Size([8, 1]), num:8
transformer: encoder.layers.0.vis_encoder.attn.precision_theta, shape=torch.Size([8, 1]), num:8
transformer: encoder.layers.0.vis_encoder.attn.q_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.0.vis_encoder.attn.q_linear.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.vis_encoder.attn.v_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.0.vis_encoder.attn.v_linear.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.vis_encoder.attn.k_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.0.vis_encoder.attn.k_linear.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.vis_encoder.attn.out.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.0.vis_encoder.attn.out.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.vis_encoder.ff.linear_1.weight, shape=torch.Size([2048, 512]), num:1048576
transformer: encoder.layers.0.vis_encoder.ff.linear_1.bias, shape=torch.Size([2048]), num:2048
transformer: encoder.layers.0.vis_encoder.ff.linear_2.weight, shape=torch.Size([512, 2048]), num:1048576
transformer: encoder.layers.0.vis_encoder.ff.linear_2.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.txt_update.norm_1.alpha, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.txt_update.norm_1.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.txt_update.norm_2.alpha, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.txt_update.norm_2.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.txt_update.norm_3.alpha, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.txt_update.norm_3.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.txt_update.attn.q_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.0.txt_update.attn.q_linear.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.txt_update.attn.v_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.0.txt_update.attn.v_linear.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.txt_update.attn.k_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.0.txt_update.attn.k_linear.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.txt_update.attn.out.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.0.txt_update.attn.out.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.txt_update.linear_q1.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.0.txt_update.linear_q2.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.0.txt_update.linear_v1.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.0.txt_update.linear_v1.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.txt_update.linear_v2.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.0.txt_update.linear_v2.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.txt_update.ff.linear_1.weight, shape=torch.Size([2048, 512]), num:1048576
transformer: encoder.layers.0.txt_update.ff.linear_1.bias, shape=torch.Size([2048]), num:2048
transformer: encoder.layers.0.txt_update.ff.linear_2.weight, shape=torch.Size([512, 2048]), num:1048576
transformer: encoder.layers.0.txt_update.ff.linear_2.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.vis_update.norm_1.alpha, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.vis_update.norm_1.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.vis_update.norm_2.alpha, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.vis_update.norm_2.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.vis_update.norm_3.alpha, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.vis_update.norm_3.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.vis_update.attn.q_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.0.vis_update.attn.q_linear.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.vis_update.attn.v_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.0.vis_update.attn.v_linear.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.vis_update.attn.k_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.0.vis_update.attn.k_linear.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.vis_update.attn.out.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.0.vis_update.attn.out.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.vis_update.linear_q1.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.0.vis_update.linear_q2.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.0.vis_update.linear_v1.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.0.vis_update.linear_v1.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.vis_update.linear_v2.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.0.vis_update.linear_v2.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.0.vis_update.ff.linear_1.weight, shape=torch.Size([2048, 512]), num:1048576
transformer: encoder.layers.0.vis_update.ff.linear_1.bias, shape=torch.Size([2048]), num:2048
transformer: encoder.layers.0.vis_update.ff.linear_2.weight, shape=torch.Size([512, 2048]), num:1048576
transformer: encoder.layers.0.vis_update.ff.linear_2.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.txt_encoder.norm_1.alpha, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.txt_encoder.norm_1.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.txt_encoder.norm_2.alpha, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.txt_encoder.norm_2.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.txt_encoder.attn.q_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.1.txt_encoder.attn.q_linear.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.txt_encoder.attn.v_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.1.txt_encoder.attn.v_linear.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.txt_encoder.attn.k_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.1.txt_encoder.attn.k_linear.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.txt_encoder.attn.out.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.1.txt_encoder.attn.out.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.txt_encoder.ff.linear_1.weight, shape=torch.Size([2048, 512]), num:1048576
transformer: encoder.layers.1.txt_encoder.ff.linear_1.bias, shape=torch.Size([2048]), num:2048
transformer: encoder.layers.1.txt_encoder.ff.linear_2.weight, shape=torch.Size([512, 2048]), num:1048576
transformer: encoder.layers.1.txt_encoder.ff.linear_2.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.vis_encoder.norm_1.alpha, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.vis_encoder.norm_1.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.vis_encoder.norm_2.alpha, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.vis_encoder.norm_2.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.vis_encoder.attn.mean_rho, shape=torch.Size([8, 1]), num:8
transformer: encoder.layers.1.vis_encoder.attn.mean_theta, shape=torch.Size([8, 1]), num:8
transformer: encoder.layers.1.vis_encoder.attn.precision_rho, shape=torch.Size([8, 1]), num:8
transformer: encoder.layers.1.vis_encoder.attn.precision_theta, shape=torch.Size([8, 1]), num:8
transformer: encoder.layers.1.vis_encoder.attn.q_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.1.vis_encoder.attn.q_linear.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.vis_encoder.attn.v_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.1.vis_encoder.attn.v_linear.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.vis_encoder.attn.k_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.1.vis_encoder.attn.k_linear.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.vis_encoder.attn.out.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.1.vis_encoder.attn.out.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.vis_encoder.ff.linear_1.weight, shape=torch.Size([2048, 512]), num:1048576
transformer: encoder.layers.1.vis_encoder.ff.linear_1.bias, shape=torch.Size([2048]), num:2048
transformer: encoder.layers.1.vis_encoder.ff.linear_2.weight, shape=torch.Size([512, 2048]), num:1048576
transformer: encoder.layers.1.vis_encoder.ff.linear_2.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.txt_update.norm_1.alpha, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.txt_update.norm_1.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.txt_update.norm_2.alpha, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.txt_update.norm_2.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.txt_update.norm_3.alpha, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.txt_update.norm_3.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.txt_update.attn.q_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.1.txt_update.attn.q_linear.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.txt_update.attn.v_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.1.txt_update.attn.v_linear.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.txt_update.attn.k_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.1.txt_update.attn.k_linear.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.txt_update.attn.out.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.1.txt_update.attn.out.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.txt_update.linear_q1.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.1.txt_update.linear_q2.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.1.txt_update.linear_v1.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.1.txt_update.linear_v1.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.txt_update.linear_v2.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.1.txt_update.linear_v2.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.txt_update.ff.linear_1.weight, shape=torch.Size([2048, 512]), num:1048576
transformer: encoder.layers.1.txt_update.ff.linear_1.bias, shape=torch.Size([2048]), num:2048
transformer: encoder.layers.1.txt_update.ff.linear_2.weight, shape=torch.Size([512, 2048]), num:1048576
transformer: encoder.layers.1.txt_update.ff.linear_2.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.vis_update.norm_1.alpha, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.vis_update.norm_1.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.vis_update.norm_2.alpha, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.vis_update.norm_2.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.vis_update.norm_3.alpha, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.vis_update.norm_3.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.vis_update.attn.q_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.1.vis_update.attn.q_linear.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.vis_update.attn.v_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.1.vis_update.attn.v_linear.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.vis_update.attn.k_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.1.vis_update.attn.k_linear.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.vis_update.attn.out.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.1.vis_update.attn.out.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.vis_update.linear_q1.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.1.vis_update.linear_q2.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.1.vis_update.linear_v1.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.1.vis_update.linear_v1.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.vis_update.linear_v2.weight, shape=torch.Size([512, 512]), num:262144
transformer: encoder.layers.1.vis_update.linear_v2.bias, shape=torch.Size([512]), num:512
transformer: encoder.layers.1.vis_update.ff.linear_1.weight, shape=torch.Size([2048, 512]), num:1048576
transformer: encoder.layers.1.vis_update.ff.linear_1.bias, shape=torch.Size([2048]), num:2048
transformer: encoder.layers.1.vis_update.ff.linear_2.weight, shape=torch.Size([512, 2048]), num:1048576
transformer: encoder.layers.1.vis_update.ff.linear_2.bias, shape=torch.Size([512]), num:512
transformer: encoder.norm.alpha, shape=torch.Size([512]), num:512
transformer: encoder.norm.bias, shape=torch.Size([512]), num:512
transformer: decoder.embed.embed.weight, shape=torch.Size([7667, 512]), num:3925504
transformer: decoder.pe.pe, shape=torch.Size([1, 200, 512]), num:102400
transformer: decoder.layers.0.norm_1.alpha, shape=torch.Size([512]), num:512
transformer: decoder.layers.0.norm_1.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.0.norm_2.alpha, shape=torch.Size([512]), num:512
transformer: decoder.layers.0.norm_2.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.0.norm_3.alpha, shape=torch.Size([512]), num:512
transformer: decoder.layers.0.norm_3.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.0.attn_1.q_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.0.attn_1.q_linear.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.0.attn_1.v_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.0.attn_1.v_linear.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.0.attn_1.k_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.0.attn_1.k_linear.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.0.attn_1.out.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.0.attn_1.out.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.0.attn_2.q_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.0.attn_2.q_linear.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.0.attn_2.v_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.0.attn_2.v_linear.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.0.attn_2.k_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.0.attn_2.k_linear.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.0.attn_2.out.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.0.attn_2.out.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.0.ff.linear_1.weight, shape=torch.Size([2048, 512]), num:1048576
transformer: decoder.layers.0.ff.linear_1.bias, shape=torch.Size([2048]), num:2048
transformer: decoder.layers.0.ff.linear_2.weight, shape=torch.Size([512, 2048]), num:1048576
transformer: decoder.layers.0.ff.linear_2.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.1.norm_1.alpha, shape=torch.Size([512]), num:512
transformer: decoder.layers.1.norm_1.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.1.norm_2.alpha, shape=torch.Size([512]), num:512
transformer: decoder.layers.1.norm_2.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.1.norm_3.alpha, shape=torch.Size([512]), num:512
transformer: decoder.layers.1.norm_3.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.1.attn_1.q_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.1.attn_1.q_linear.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.1.attn_1.v_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.1.attn_1.v_linear.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.1.attn_1.k_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.1.attn_1.k_linear.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.1.attn_1.out.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.1.attn_1.out.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.1.attn_2.q_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.1.attn_2.q_linear.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.1.attn_2.v_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.1.attn_2.v_linear.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.1.attn_2.k_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.1.attn_2.k_linear.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.1.attn_2.out.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.1.attn_2.out.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.1.ff.linear_1.weight, shape=torch.Size([2048, 512]), num:1048576
transformer: decoder.layers.1.ff.linear_1.bias, shape=torch.Size([2048]), num:2048
transformer: decoder.layers.1.ff.linear_2.weight, shape=torch.Size([512, 2048]), num:1048576
transformer: decoder.layers.1.ff.linear_2.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.2.norm_1.alpha, shape=torch.Size([512]), num:512
transformer: decoder.layers.2.norm_1.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.2.norm_2.alpha, shape=torch.Size([512]), num:512
transformer: decoder.layers.2.norm_2.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.2.norm_3.alpha, shape=torch.Size([512]), num:512
transformer: decoder.layers.2.norm_3.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.2.attn_1.q_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.2.attn_1.q_linear.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.2.attn_1.v_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.2.attn_1.v_linear.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.2.attn_1.k_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.2.attn_1.k_linear.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.2.attn_1.out.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.2.attn_1.out.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.2.attn_2.q_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.2.attn_2.q_linear.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.2.attn_2.v_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.2.attn_2.v_linear.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.2.attn_2.k_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.2.attn_2.k_linear.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.2.attn_2.out.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.2.attn_2.out.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.2.ff.linear_1.weight, shape=torch.Size([2048, 512]), num:1048576
transformer: decoder.layers.2.ff.linear_1.bias, shape=torch.Size([2048]), num:2048
transformer: decoder.layers.2.ff.linear_2.weight, shape=torch.Size([512, 2048]), num:1048576
transformer: decoder.layers.2.ff.linear_2.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.3.norm_1.alpha, shape=torch.Size([512]), num:512
transformer: decoder.layers.3.norm_1.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.3.norm_2.alpha, shape=torch.Size([512]), num:512
transformer: decoder.layers.3.norm_2.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.3.norm_3.alpha, shape=torch.Size([512]), num:512
transformer: decoder.layers.3.norm_3.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.3.attn_1.q_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.3.attn_1.q_linear.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.3.attn_1.v_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.3.attn_1.v_linear.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.3.attn_1.k_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.3.attn_1.k_linear.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.3.attn_1.out.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.3.attn_1.out.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.3.attn_2.q_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.3.attn_2.q_linear.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.3.attn_2.v_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.3.attn_2.v_linear.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.3.attn_2.k_linear.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.3.attn_2.k_linear.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.3.attn_2.out.weight, shape=torch.Size([512, 512]), num:262144
transformer: decoder.layers.3.attn_2.out.bias, shape=torch.Size([512]), num:512
transformer: decoder.layers.3.ff.linear_1.weight, shape=torch.Size([2048, 512]), num:1048576
transformer: decoder.layers.3.ff.linear_1.bias, shape=torch.Size([2048]), num:2048
transformer: decoder.layers.3.ff.linear_2.weight, shape=torch.Size([512, 2048]), num:1048576
transformer: decoder.layers.3.ff.linear_2.bias, shape=torch.Size([512]), num:512
transformer: decoder.norm.alpha, shape=torch.Size([512]), num:512
transformer: decoder.norm.bias, shape=torch.Size([512]), num:512
transformer: logit.weight, shape=torch.Size([7667, 512]), num:3925504
transformer: logit.bias, shape=torch.Size([7667]), num:7667
vis_enc: fc.weight, shape=torch.Size([1024, 2048]), num:2097152
vis_enc: fc.bias, shape=torch.Size([1024]), num:1024
txt_enc: embedding.we.weight, shape=torch.Size([7667, 300]), num:2300100
txt_enc: rnn.weight_ih_l0, shape=torch.Size([1536, 300]), num:460800
txt_enc: rnn.weight_hh_l0, shape=torch.Size([1536, 512]), num:786432
txt_enc: rnn.bias_ih_l0, shape=torch.Size([1536]), num:1536
txt_enc: rnn.bias_hh_l0, shape=torch.Size([1536]), num:1536
txt_enc: rnn.weight_ih_l0_reverse, shape=torch.Size([1536, 300]), num:460800
txt_enc: rnn.weight_hh_l0_reverse, shape=torch.Size([1536, 512]), num:786432
txt_enc: rnn.bias_ih_l0_reverse, shape=torch.Size([1536]), num:1536
txt_enc: rnn.bias_hh_l0_reverse, shape=torch.Size([1536]), num:1536
txt_enc: fc.weight, shape=torch.Size([1024, 1024]), num:1048576
txt_enc: fc.bias, shape=torch.Size([1024]), num:1024
src_enc: embedding.we.weight, shape=torch.Size([7229, 300]), num:2168700
src_enc: rnn.weight_ih_l0, shape=torch.Size([1536, 300]), num:460800
src_enc: rnn.weight_hh_l0, shape=torch.Size([1536, 512]), num:786432
src_enc: rnn.bias_ih_l0, shape=torch.Size([1536]), num:1536
src_enc: rnn.bias_hh_l0, shape=torch.Size([1536]), num:1536
src_enc: rnn.weight_ih_l0_reverse, shape=torch.Size([1536, 300]), num:460800
src_enc: rnn.weight_hh_l0_reverse, shape=torch.Size([1536, 512]), num:786432
src_enc: rnn.bias_ih_l0_reverse, shape=torch.Size([1536]), num:1536
src_enc: rnn.bias_hh_l0_reverse, shape=torch.Size([1536]), num:1536
src_enc: fc.weight, shape=torch.Size([1024, 1024]), num:1048576
src_enc: fc.bias, shape=torch.Size([1024]), num:1024
tgt_enc: embedding.we.weight, shape=torch.Size([7667, 300]), num:2300100
tgt_enc: rnn.weight_ih_l0, shape=torch.Size([1536, 300]), num:460800
tgt_enc: rnn.weight_hh_l0, shape=torch.Size([1536, 512]), num:786432
tgt_enc: rnn.bias_ih_l0, shape=torch.Size([1536]), num:1536
tgt_enc: rnn.bias_hh_l0, shape=torch.Size([1536]), num:1536
tgt_enc: rnn.weight_ih_l0_reverse, shape=torch.Size([1536, 300]), num:460800
tgt_enc: rnn.weight_hh_l0_reverse, shape=torch.Size([1536, 512]), num:786432
tgt_enc: rnn.bias_ih_l0_reverse, shape=torch.Size([1536]), num:1536
tgt_enc: rnn.bias_hh_l0_reverse, shape=torch.Size([1536]), num:1536
tgt_enc: fc.weight, shape=torch.Size([1024, 1024]), num:1048576
tgt_enc: fc.bias, shape=torch.Size([1024]), num:1024
num params 319, num weights 78571319
trainable: num params 281, num weights 54923315